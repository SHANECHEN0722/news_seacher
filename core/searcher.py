"""
æ–°é—»æœç´¢æ¨¡å—
"""
import requests
import time
from bs4 import BeautifulSoup
from config import SEARCH_HEADERS, GOOGLE_COOKIE


class NewsSearcher:
    """æ–°é—»æœç´¢å™¨"""
    
    @staticmethod
    def search_baidu(keyword, max_results=10):
        """ä½¿ç”¨ç™¾åº¦æ–°é—»æœç´¢"""
        news_links = []
        
        try:
            print(f"ğŸ” [Baidu] æ­£åœ¨æœç´¢: {keyword}")
            
            search_url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={keyword}"
            response = requests.get(search_url, headers=SEARCH_HEADERS, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = soup.find_all('div', class_='result')
            
            if not results:
                results = soup.find_all('div', class_='c-container')
            
            count = 0
            for result in results:
                if count >= max_results:
                    break
                
                link_tag = result.find('a')
                if link_tag and link_tag.get('href'):
                    url = link_tag.get('href')
                    if 'baidu.com' not in url or 'baijiahao.baidu.com' in url:
                        news_links.append(url)
                        print(f"âœ… æ‰¾åˆ°é“¾æ¥ {count + 1}: {url[:80]}...")
                        count += 1
            
            return news_links
            
        except Exception as e:
            print(f"âš ï¸ ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    @staticmethod
    def search_google(keyword, max_results=10):
        """ä½¿ç”¨ Google æœç´¢ï¼ˆæ”¯æŒ Cookieï¼‰"""
        news_links = []
        
        if not GOOGLE_COOKIE:
            print("âš ï¸ æœªé…ç½® GOOGLE_COOKIEï¼Œè·³è¿‡ Google æœç´¢")
            return []
        
        try:
            print(f"ğŸ” [Google] æ­£åœ¨æœç´¢: {keyword}")
            
            # Google æœç´¢ URL
            search_query = f"{keyword} æ–°é—»"
            search_url = f"https://www.google.com.hk/search?q={requests.utils.quote(search_query)}&num={max_results}&hl=zh-CN"
            
            # æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´ï¼ˆåŒ…å« Cookieï¼‰
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cookie': GOOGLE_COOKIE,
                'Referer': 'https://www.google.com/',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'max-age=0'
            }
            
            # å‘é€è¯·æ±‚
            response = requests.get(search_url, headers=headers, timeout=15)
            
            if response.status_code != 200:
                print(f"âš ï¸ Google è¿”å›çŠ¶æ€ç : {response.status_code}")
                return []
            
            # è§£æ HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾ class="yuRUbf" çš„ divï¼ˆGoogle æœç´¢ç»“æœå®¹å™¨ï¼‰
            count = 0
            for div in soup.find_all('div', class_='yuRUbf'):
                if count >= max_results:
                    break
                
                a_tag = div.find('a', href=True)
                if a_tag:
                    url = a_tag['href']
                    if url.startswith('http') and not any(x in url for x in [
                        'google.com',
                        'youtube.com',
                        'webcache.googleusercontent.com'
                    ]):
                        news_links.append(url)
                        print(f"âœ… æ‰¾åˆ°é“¾æ¥ {count + 1}: {url[:80]}...")
                        count += 1
            
            # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰ <a> æ ‡ç­¾
            if not news_links:
                print("âš ï¸ å°è¯•å¤‡ç”¨è§£ææ–¹å¼...")
                for link in soup.find_all('a', href=True):
                    if count >= max_results:
                        break
                    
                    href = link['href']
                    
                    # æå–ä»¥ /url?q= å¼€å¤´çš„é“¾æ¥
                    if href.startswith('/url?q='):
                        url = href.split('/url?q=')[1].split('&')[0]
                        url = requests.utils.unquote(url)
                        
                        if url.startswith('http') and not any(x in url for x in [
                            'google.com',
                            'youtube.com',
                            'webcache.googleusercontent.com'
                        ]):
                            news_links.append(url)
                            print(f"âœ… æ‰¾åˆ°é“¾æ¥ {count + 1}: {url[:80]}...")
                            count += 1
            
            return news_links
            
        except Exception as e:
            print(f"âš ï¸ Google æœç´¢å¤±è´¥: {e}")
            return []
    
    @staticmethod
    def search_bing(keyword, max_results=10):
        """ä½¿ç”¨ Bing æœç´¢ï¼ˆæ›´å‹å¥½çš„åçˆ¬è™«ç­–ç•¥ï¼‰"""
        news_links = []
        
        try:
            print(f"ğŸ” [Bing] æ­£åœ¨æœç´¢: {keyword}")
            
            # Bing æœç´¢ URL
            search_query = f"{keyword} æ–°é—»"
            search_url = f"https://www.bing.com/search?q={requests.utils.quote(search_query)}&count={max_results * 2}&setlang=zh-CN"
            
            # æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Referer': 'https://www.bing.com/'
            }
            
            # å‘é€è¯·æ±‚
            response = requests.get(search_url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"âš ï¸ Bing è¿”å›çŠ¶æ€ç : {response.status_code}")
                return []
            
            # è§£æ HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Bing æœç´¢ç»“æœé€šå¸¸åœ¨ <li class="b_algo"> ä¸­
            count = 0
            for result in soup.find_all('li', class_='b_algo'):
                if count >= max_results:
                    break
                
                # æŸ¥æ‰¾é“¾æ¥
                link = result.find('a', href=True)
                if link:
                    url = link['href']
                    
                    # è¿‡æ»¤æ‰ä¸ç›¸å…³çš„é“¾æ¥
                    if url and url.startswith('http') and not any(x in url for x in [
                        'bing.com',
                        'microsoft.com',
                        'youtube.com'
                    ]):
                        news_links.append(url)
                        print(f"âœ… æ‰¾åˆ°é“¾æ¥ {count + 1}: {url[:80]}...")
                        count += 1
            
            return news_links
            
        except Exception as e:
            print(f"âš ï¸ Bing æœç´¢å¤±è´¥: {e}")
            return []


    @classmethod
    def search(cls, keyword, max_results=10, timelimit='a'):
        """
        ç»¼åˆæœç´¢ï¼ˆæ™ºèƒ½ç»„åˆå¤šä¸ªæœç´¢å¼•æ“ï¼‰
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨ Googleï¼ˆå¦‚æœé…ç½®äº† GOOGLE_COOKIEï¼‰
        2. å¦‚æœæ•°é‡ä¸å¤Ÿï¼Œç”¨ç™¾åº¦è¡¥å……
        3. å¦‚æœè¿˜ä¸å¤Ÿï¼Œç”¨ Bing è¡¥å……
        4. è‡ªåŠ¨å»é‡ï¼Œç¡®ä¿é“¾æ¥å”¯ä¸€
        """
        all_links = []
        seen_urls = set()  # ç”¨äºå»é‡
        
        # éš¾ä»¥çˆ¬å–çš„ç½‘ç«™é»‘åå•
        blocked_domains = [
            'zhihu.com', 'weibo.com', 'twitter.com', 'facebook.com',
            'instagram.com', 'youtube.com', 'bilibili.com', 'douyin.com'
        ]
        
        def is_valid_url(url):
            """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆï¼ˆä¸åœ¨é»‘åå•ä¸­ï¼‰"""
            return not any(domain in url for domain in blocked_domains)
        
        def add_unique_links(new_links):
            """æ·»åŠ é“¾æ¥å¹¶å»é‡"""
            added = 0
            for url in new_links:
                if url not in seen_urls and len(all_links) < max_results and is_valid_url(url):
                    all_links.append(url)
                    seen_urls.add(url)
                    added += 1
                elif not is_valid_url(url):
                    print(f"   âš ï¸ è¿‡æ»¤é»‘åå•ç½‘ç«™: {url[:50]}...")
            return added
        
        # 1. å¦‚æœé…ç½®äº† Google Cookieï¼Œä¼˜å…ˆä½¿ç”¨ Google
        if GOOGLE_COOKIE:
            print(f"ğŸ” [1/3] ä½¿ç”¨ Google æœç´¢ï¼ˆç›®æ ‡: {max_results} ç¯‡ï¼‰...")
            google_links = cls.search_google(keyword, max_results)
            added = add_unique_links(google_links)
            print(f"   âœ… Google æ‰¾åˆ° {added} ç¯‡ï¼Œå½“å‰æ€»æ•°: {len(all_links)}/{max_results}")
        
        # 2. å¦‚æœæ•°é‡ä¸å¤Ÿï¼Œä½¿ç”¨ç™¾åº¦è¡¥å……
        if len(all_links) < max_results:
            remaining = max_results - len(all_links)
            print(f"ğŸ” [2/3] ä½¿ç”¨ç™¾åº¦è¡¥å……ï¼ˆè¿˜éœ€: {remaining} ç¯‡ï¼‰...")
            baidu_links = cls.search_baidu(keyword, remaining * 2)  # å¤šæœä¸€äº›ï¼Œå› ä¸ºå¯èƒ½æœ‰é‡å¤
            added = add_unique_links(baidu_links)
            print(f"   âœ… ç™¾åº¦è¡¥å…… {added} ç¯‡ï¼Œå½“å‰æ€»æ•°: {len(all_links)}/{max_results}")
        
        # 3. å¦‚æœè¿˜ä¸å¤Ÿï¼Œä½¿ç”¨ Bing è¡¥å……
        if len(all_links) < max_results:
            remaining = max_results - len(all_links)
            print(f"ğŸ” [3/3] ä½¿ç”¨ Bing è¡¥å……ï¼ˆè¿˜éœ€: {remaining} ç¯‡ï¼‰...")
            bing_links = cls.search_bing(keyword, remaining * 2)
            added = add_unique_links(bing_links)
            print(f"   âœ… Bing è¡¥å…… {added} ç¯‡ï¼Œå½“å‰æ€»æ•°: {len(all_links)}/{max_results}")
        
        # æœ€ç»ˆç»“æœ
        if all_links:
            print(f"\nğŸ‰ æœç´¢å®Œæˆï¼å…±æ‰¾åˆ° {len(all_links)} ç¯‡æ–‡ç« ")
        else:
            print("\nâŒ æ‰€æœ‰æœç´¢å¼•æ“éƒ½æœªæ‰¾åˆ°ç»“æœ")
        
        return all_links
