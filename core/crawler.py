"""
æ–°é—»çˆ¬å–æ¨¡å—
"""
from newspaper import Article
from fuzzywuzzy import fuzz
from config import SIMILARITY_THRESHOLD


class NewsCrawler:
    """æ–°é—»çˆ¬è™«"""
    
    @staticmethod
    def crawl_article(url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        try:
            article = Article(url, language='zh')
            article.download()
            article.parse()
            
            if len(article.text) > 200:
                return {
                    "url": url,
                    "title": article.title,
                    "text": article.text
                }
        except Exception as e:
            print(f" [!] çˆ¬å–å¤±è´¥: {url} - {e}")
        
        return None
    
    @classmethod
    def crawl_articles(cls, urls):
        """æ‰¹é‡çˆ¬å–æ–‡ç« """
        articles = []
        
        for url in urls:
            article = cls.crawl_article(url)
            if article:
                articles.append(article)
        
        return articles
    
    @staticmethod
    def deduplicate(articles):
        """ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…å»é‡"""
        unique_articles = []
        seen_titles = []
        
        print(f"ğŸ” [Cleaning] æ­£åœ¨å»é‡å¤„ç† {len(articles)} ç¯‡æ–‡ç« ...")
        
        for article in articles:
            is_duplicate = False
            
            for seen_title in seen_titles:
                similarity = fuzz.token_sort_ratio(article['title'], seen_title)
                if similarity > SIMILARITY_THRESHOLD:
                    is_duplicate = True
                    print(f" [!] å‰”é™¤é‡å¤å†…å®¹ (ç›¸ä¼¼åº¦ {similarity}%): {article['title']}")
                    break
            
            if not is_duplicate:
                unique_articles.append(article)
                seen_titles.append(article['title'])
        
        print(f"âœ… [Cleaning] å»é‡å®Œæˆ. å‰©ä½™ {len(unique_articles)} ç¯‡ç‹¬ç«‹æ–‡ç« .")
        return unique_articles
