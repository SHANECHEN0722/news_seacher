"""
æ–°é—»çˆ¬å–æ¨¡å—
"""
from newspaper import Article
from fuzzywuzzy import fuzz
from config import SIMILARITY_THRESHOLD


class NewsCrawler:
    """æ–°é—»çˆ¬è™«"""
    
    # éš¾ä»¥çˆ¬å–çš„ç½‘ç«™é»‘åå•
    BLOCKED_DOMAINS = [
        # 'zhihu.com',
        # 'weibo.com',
        # 'twitter.com',
        # 'facebook.com',
        # 'instagram.com',
        # 'youtube.com',
        # 'bilibili.com',
        # 'douyin.com'
    ]
    
    @classmethod
    def is_blocked_domain(cls, url):
        """æ£€æŸ¥URLæ˜¯å¦åœ¨é»‘åå•ä¸­"""
        return any(domain in url for domain in cls.BLOCKED_DOMAINS)
    
    @classmethod
    def crawl_article(cls, url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        # è¿‡æ»¤é»‘åå•ç½‘ç«™
        if cls.is_blocked_domain(url):
            print(f" [!] è·³è¿‡é»‘åå•ç½‘ç«™: {url}")
            return None
        
        try:
            print(f" [â†’] æ­£åœ¨çˆ¬å–: {url}")
            article = Article(url)  # ä¸æŒ‡å®šè¯­è¨€ï¼Œè®© newspaper è‡ªåŠ¨æ£€æµ‹
            article.download()
            article.parse()
            
            if len(article.text) > 200:
                print(f" [âœ“] æˆåŠŸ: {article.title[:50]}... ({len(article.text)} å­—)")
                return {
                    "url": url,
                    "title": article.title,
                    "text": article.text
                }
            else:
                print(f" [!] å†…å®¹å¤ªçŸ­ ({len(article.text)} å­—): {url}")
        except Exception as e:
            print(f" [!] çˆ¬å–å¤±è´¥: {url}")
            print(f"     é”™è¯¯: {str(e)[:100]}")
        
        return None
    
    @classmethod
    def crawl_articles(cls, urls, use_dynamic=False):
        """
        æ‰¹é‡çˆ¬å–æ–‡ç« 
        
        Args:
            urls: URLåˆ—è¡¨
            use_dynamic: æ˜¯å¦ä½¿ç”¨åŠ¨æ€çˆ¬è™«ï¼ˆSeleniumï¼‰
        """
        articles = []
        failed_urls = []
        
        # ç¬¬ä¸€è½®ï¼šä½¿ç”¨é™æ€çˆ¬è™«
        for url in urls:
            article = cls.crawl_article(url)
            if article:
                articles.append(article)
            else:
                failed_urls.append(url)
        
        # ç¬¬äºŒè½®ï¼šå¦‚æœå¯ç”¨åŠ¨æ€çˆ¬è™«ä¸”æœ‰å¤±è´¥çš„URLï¼Œå°è¯•ç”¨Selenium
        if use_dynamic and failed_urls:
            print(f"\nğŸ”„ [Dynamic] å°è¯•ç”¨åŠ¨æ€çˆ¬è™«é‡æ–°çˆ¬å– {len(failed_urls)} ä¸ªå¤±è´¥çš„é“¾æ¥...")
            try:
                from core.dynamic_crawler import DynamicCrawler
                
                for url in failed_urls:
                    article = DynamicCrawler.crawl_article(url)
                    if article:
                        articles.append(article)
            except ImportError:
                print("âš ï¸ åŠ¨æ€çˆ¬è™«æœªå®‰è£…ï¼Œè·³è¿‡ã€‚è¿è¡Œ: pip install selenium")
            except Exception as e:
                print(f"âš ï¸ åŠ¨æ€çˆ¬è™«å¤±è´¥: {e}")
        
        return articles
    
    @staticmethod
    def deduplicate(articles):
        """ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…å»é‡"""
        unique_articles = []
        seen_titles = []
        
        print(f"ğŸ” [Cleaning] æ­£åœ¨å»é‡å¤„ç† {len(articles)} ç¯‡æ–‡ç« ...")
        
        for article in articles:
            is_duplicate = False
            
            for seen_title in seen_titles:
                similarity = fuzz.token_sort_ratio(article['title'], seen_title)
                if similarity > SIMILARITY_THRESHOLD:
                    is_duplicate = True
                    print(f" [!] å‰”é™¤é‡å¤å†…å®¹ (ç›¸ä¼¼åº¦ {similarity}%): {article['title']}")
                    break
            
            if not is_duplicate:
                unique_articles.append(article)
                seen_titles.append(article['title'])
        
        print(f"âœ… [Cleaning] å»é‡å®Œæˆ. å‰©ä½™ {len(unique_articles)} ç¯‡ç‹¬ç«‹æ–‡ç« .")
        return unique_articles
