"""
åŠ¨æ€ç½‘é¡µçˆ¬å–æ¨¡å—ï¼ˆæ”¯æŒJavaScriptæ¸²æŸ“ï¼‰
ä½¿ç”¨ Selenium + Chrome æ— å¤´æµè§ˆå™¨
"""
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup


class DynamicCrawler:
    """åŠ¨æ€ç½‘é¡µçˆ¬è™«ï¼ˆæ”¯æŒJavaScriptï¼‰"""
    
    @staticmethod
    def setup_driver():
        """é…ç½®Chromeæ— å¤´æµè§ˆå™¨"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
        
        try:
            # å°è¯•ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç†é©±åŠ¨
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
                driver = webdriver.Chrome(service=service, options=chrome_options)
                print("âœ… ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç†é©±åŠ¨")
            except ImportError:
                # å¦‚æœæ²¡å®‰è£… webdriver-managerï¼Œä½¿ç”¨ç³»ç»Ÿçš„ chromedriver
                driver = webdriver.Chrome(options=chrome_options)
                print("âœ… ä½¿ç”¨ç³»ç»Ÿ chromedriver")
            
            return driver
        except Exception as e:
            print(f"âš ï¸ Chromeé©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·è¿è¡Œ: pip install selenium webdriver-manager")
            return None
    
    @classmethod
    def crawl_article(cls, url, wait_time=3):
        """
        çˆ¬å–åŠ¨æ€ç½‘é¡µ
        
        Args:
            url: ç½‘é¡µURL
            wait_time: ç­‰å¾…JavaScriptåŠ è½½çš„æ—¶é—´ï¼ˆç§’ï¼‰
        """
        driver = None
        try:
            print(f" [â†’] æ­£åœ¨çˆ¬å–ï¼ˆåŠ¨æ€ï¼‰: {url}")
            
            driver = cls.setup_driver()
            if not driver:
                return None
            
            # è®¿é—®é¡µé¢
            driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(wait_time)
            
            # å°è¯•ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½ï¼ˆå¤šç§å¯èƒ½çš„æ ‡ç­¾ï¼‰
            try:
                WebDriverWait(driver, 10).until(
                    lambda d: d.find_element(By.TAG_NAME, "article") or 
                             d.find_element(By.TAG_NAME, "main") or
                             d.find_elements(By.TAG_NAME, "p")
                )
            except:
                pass  # ç»§ç»­
            
            # é¢å¤–ç­‰å¾…ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ
            time.sleep(2)
            
            # è·å–é¡µé¢æºç 
            html = driver.page_source
            
            # è°ƒè¯•ï¼šä¿å­˜HTMLçœ‹çœ‹
            # with open('/tmp/debug_page.html', 'w', encoding='utf-8') as f:
            #     f.write(html)
            # print(f"     è°ƒè¯•ï¼šHTMLå·²ä¿å­˜åˆ° /tmp/debug_page.html")
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = ""
            title_tags = ['h1', 'h2', 'title']
            for tag in title_tags:
                title_elem = soup.find(tag)
                if title_elem and title_elem.get_text().strip():
                    title = title_elem.get_text().strip()
                    break
            
            # æå–æ­£æ–‡ - ç®€å•ç²—æš´çš„æ–¹æ³•
            text = ""
            
            # æ–¹æ³•1ï¼šè·å–æ‰€æœ‰æ®µè½
            paragraphs = soup.find_all('p')
            if paragraphs:
                para_text = '\n'.join([p.get_text(strip=True) for p in paragraphs])
                if len(para_text) > 200:
                    text = para_text
            
            # æ–¹æ³•2ï¼šå¦‚æœæ®µè½ä¸å¤Ÿï¼Œç›´æ¥è·å–body
            if len(text) < 200:
                body = soup.find('body')
                if body:
                    # åªç§»é™¤scriptå’Œstyle
                    for script in body(['script', 'style']):
                        script.decompose()
                    text = body.get_text(separator=' ', strip=True)
            
            # ç®€å•æ¸…ç†ï¼šç§»é™¤å¤šä½™ç©ºæ ¼
            text = ' '.join(text.split())
            
            if len(text) > 200:
                print(f" [âœ“] æˆåŠŸï¼ˆåŠ¨æ€ï¼‰: {title[:50]}... ({len(text)} å­—)")
                return {
                    "url": url,
                    "title": title or "æ— æ ‡é¢˜",
                    "text": text
                }
            else:
                print(f" [!] å†…å®¹å¤ªçŸ­ ({len(text)} å­—): {url}")
                # è°ƒè¯•ï¼šæ˜¾ç¤ºæŠ“åˆ°çš„å†…å®¹å‰100å­—
                if text:
                    print(f"     æŠ“åˆ°çš„å†…å®¹: {text[:100]}...")
                return None
                
        except Exception as e:
            print(f" [!] åŠ¨æ€çˆ¬å–å¤±è´¥: {url}")
            print(f"     é”™è¯¯: {str(e)[:100]}")
            return None
        finally:
            if driver:
                driver.quit()
    
    @classmethod
    def crawl_articles(cls, urls, wait_time=3):
        """æ‰¹é‡çˆ¬å–åŠ¨æ€ç½‘é¡µ"""
        articles = []
        
        for url in urls:
            article = cls.crawl_article(url, wait_time)
            if article:
                articles.append(article)
        
        return articles
